<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://al-folio.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://al-folio.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-06-01T13:27:22+00:00</updated><id>https://al-folio.github.io/feed.xml</id><title type="html">mAI alignment lab</title><subtitle>Junior Research Group at University of Bonn focusing on AI alignment and safety issues.
</subtitle><entry><title type="html">Evaluating the Paperclip Maximizer and Instrumental Goals</title><link href="https://al-folio.github.io/blog/2025/paperclip-maximizer/" rel="alternate" type="text/html" title="Evaluating the Paperclip Maximizer and Instrumental Goals" /><published>2025-05-21T00:00:00+00:00</published><updated>2025-05-21T00:00:00+00:00</updated><id>https://al-folio.github.io/blog/2025/paperclip-maximizer</id><content type="html" xml:base="https://al-folio.github.io/blog/2025/paperclip-maximizer/"><![CDATA[<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2502.12206">Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?</a><br />
<strong>Authors:</strong> Yufei He, Yuexin Li, Jiaying Wu, Yuan Sui, Yulin Chen, Bryan Hooi<br />
<strong>Session Date:</strong> May 21, 2025</p>

<h2 id="abstract">Abstract</h2>

<p>As large language models (LLMs) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is instrumental convergence, where an AI system, in optimizing for a given objective, develops unintended intermediate goals that override the ultimate objective and deviate from human-intended goals. This issue is particularly relevant in reinforcement learning (RL)-trained models, which can generate creative but unintended strategies to maximize rewards. In this paper, we explore instrumental convergence in LLMs by comparing models trained with direct RL optimization (e.g., the o1 model) to those trained with reinforcement learning from human feedback (RLHF). We hypothesize that RL-driven models exhibit a stronger tendency for instrumental convergence due to their optimization of goal-directed behavior in ways that may misalign with human intentions. To assess this, we introduce InstrumentalEval, a benchmark for evaluating instrumental convergence in RL-trained LLMs. Initial experiments reveal cases where a model tasked with making money unexpectedly pursues instrumental objectives, such as self-replication, implying signs of instrumental convergence.</p>

<h2 id="reading-group-reflections">Reading Group Reflections</h2>

<p>By trying to make instrumental convergence quantifiable, the paper makes an important contribution to AI safety. However, the experimental setup is not entirely convincing for several reasons.</p>

<p>When trying to answer the question of whether RL training makes models lean towards instrumental convergence, we should control for other factors. However, the paper doesn’t do this, since they never train a reasoning model themselves in order to exert this control. Granted, this is difficult to do because training a model is so expensive, but this limitation should at least be discussed.</p>

<p>It is also not clear why the paper doesn’t strictly compare an RLHF model to its respective thinking model. While we do have a comparison between GPT-4o and o1, which is likely based on GPT-4o, we have no hard evidence that these models are comparable. But assuming that they are, the paper doesn’t compare o1-mini to GPT-4o-mini either, showing inconsistency, and it doesn’t compare Gemini 2.0 Flash Thinking to Gemini 2.0 Flash. Moreover, Sonnet-3.5 has no counter-part at all, so should’ve been left out. The only direct comparison that makes sense is DeepSeek-V3 with DeepSeek-R1, which we know is based on DeepSeek-V3 from public information.</p>

<p>The provided task examples seem to be nudging the models into the direction of instrumental convergence (e.g. suggesting an off-script solution in Figure 1), raising the question to what extent the shown behavior is actually a result of instrumental convergence or rather instruction-following.</p>

<p>Nonetheless, the results are very consistently showing higher instrumental convergence rates for reasoning models than for RLHF models. This suggests that the problem of RL causing the models to learn potentially undesired behavior is very real. We believe it’s one of the most urgent AI safety issues we have today.</p>]]></content><author><name></name></author><category term="safety-reading-group" /><category term="reading-group" /><category term="ai-safety" /><category term="instrumental-goals" /><category term="reinforcement-learning" /><summary type="html"><![CDATA[Investigating whether RL-based language models exhibit concerning instrumental goal-seeking behavior]]></summary></entry><entry><title type="html">Gradual Disempowerment and Systemic AI Risks</title><link href="https://al-folio.github.io/blog/2025/gradual-disempowerment/" rel="alternate" type="text/html" title="Gradual Disempowerment and Systemic AI Risks" /><published>2025-05-07T00:00:00+00:00</published><updated>2025-05-07T00:00:00+00:00</updated><id>https://al-folio.github.io/blog/2025/gradual-disempowerment</id><content type="html" xml:base="https://al-folio.github.io/blog/2025/gradual-disempowerment/"><![CDATA[<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2501.16946">Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development</a><br />
<strong>Authors:</strong> Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud<br />
<strong>Session Date:</strong> May 7, 2025</p>

<h2 id="abstract">Abstract</h2>

<p>This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment’, in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human interests that often arise from societal systems’ reliance on human participation to function. Furthermore, to the extent that these systems incentivise outcomes that do not line up with human preferences, AIs may optimize for those outcomes more aggressively. These effects may be mutually reinforcing across different domains: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.</p>

<h2 id="reading-group-reflections">Reading Group Reflections</h2>

<p>The paper gives a scary outlook on what might happen as AI slowly but surely becomes part of all aspects of our lives. It gives plausible arguments for these scenarios to happen across the economy, culture, and political systems.</p>

<p>However, some of us thought they were too pessimistic at times. For example, the paper states that the economy would not only be run BY the AIs, but increasingly also FOR the AIs. But it is not clear what an economy that produces for AIs could look like? Do we find it plausible that at some point, the only economic outputs are more GPUs and energy to power them? If AIs produced less and less for humans, wouldn’t this quickly result in backlash and respective political change?</p>

<p>However, some of the arguments are difficult to argue away. For instance, it is clear that AIs could easily become the sole producers of culture and policy as humans slowly get more and more comfortable with their (probably very convincing) output. The risk that this might cause a value lock-in is very real.</p>]]></content><author><name></name></author><category term="safety-reading-group" /><category term="reading-group" /><category term="ai-safety" /><category term="existential-risk" /><category term="disempowerment" /><summary type="html"><![CDATA[Examining how incremental AI development might lead to gradual but systemic disempowerment]]></summary></entry><entry><title type="html">Dynamic Normativity and Value Alignment</title><link href="https://al-folio.github.io/blog/2025/dynamic-normativity/" rel="alternate" type="text/html" title="Dynamic Normativity and Value Alignment" /><published>2025-04-09T00:00:00+00:00</published><updated>2025-04-09T00:00:00+00:00</updated><id>https://al-folio.github.io/blog/2025/dynamic-normativity</id><content type="html" xml:base="https://al-folio.github.io/blog/2025/dynamic-normativity/"><![CDATA[<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2406.11039">Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment</a><br />
<strong>Authors:</strong> Nicholas Kluge Corrêa 
<strong>Session Date:</strong> April 9, 2025</p>

<h2 id="abstract">Abstract</h2>

<p>The critical inquiry pervading the realm of Philosophy, and perhaps extending its influence across all Humanities disciplines, revolves around the intricacies of morality and normativity. Surprisingly, in recent years, this thematic thread has woven its way into an unexpected domain, one not conventionally associated with pondering “what ought to be”: the field of artificial intelligence (AI) research. Central to morality and AI, we find “alignment”, a problem related to the challenges of expressing human goals and values in a manner that artificial systems can follow without leading to unwanted adversarial effects. More explicitly and with our current paradigm of AI development in mind, we can think of alignment as teaching human values to non-anthropomorphic entities trained through opaque, gradient-based learning techniques. This work addresses alignment as a technical-philosophical problem that requires solid philosophical foundations and practical implementations that bring normative theory to AI system development. To accomplish this, we propose two sets of necessary and sufficient conditions that, we argue, should be considered in any alignment process. While necessary conditions serve as metaphysical and metaethical roots that pertain to the permissibility of alignment, sufficient conditions establish a blueprint for aligning AI systems under a learning-based paradigm. After laying such foundations, we present implementations of this approach by using state-of-the-art techniques and methods for aligning general-purpose language systems. We call this framework Dynamic Normativity. Its central thesis is that any alignment process under a learning paradigm that cannot fulfill its necessary and sufficient conditions will fail in producing aligned systems.</p>

<h2 id="reading-group-reflections">Reading Group Reflections</h2>

<p>The author, <a href="https://nkluge-correa.github.io/">Nicholas Kluge Corrêa</a>, philosopher and deep learning practitioner from the Center for Science and Thought at Uni Bonn, presented the core of his PhD thesis to us. Rather than viewing the conditions presented in the thesis as strictly necessary and sufficient conditions for alignment, they should be viewed as philosophical conditions and practical guidelines.
The necessary conditions revolve around assumptions of what intelligence is, how human behavior is driven by intentions, how intentions result from normative preferences, and how human preferences can be observed in the environment.
These assumptions seemed appropriate to make to us with mostly technical backgrounds, although Nicholas was quick to point out that philosophers can find them very disagreeable.
The technical guidelines stress the importance of coherently aggregating human preferences under uncertainty, making this a part of the objective function of AI systems, and implementing additional safety guardrails to mitigate unintended consequences.
These guidelines certainly make sense – they are what’s underpinning the platforms offering large language models today: For reinforcement learning from human feedback, we first learn a reward function that aggregates human preferences, and then we make that reward function a part of the training objective. Input and output filtering mechanism serve as additional guardrails to prevent misuse of the system or catches cases where the system goes off-rails.</p>

<p>So if present-day systems already follow the sufficient conditions for AI alignment, are all problems solved? Not quite. While humans impregnate the environment with their preferences, we can only hope to extract preferences that have been expressed in past environments, but not in future ones. As AIs are becoming part of our world and take over increasingly more sophisticated tasks, our preferences become highly uncertain to the point that it is difficult to extract a signal from.
Alignment isn’t solved yet.</p>]]></content><author><name></name></author><category term="safety-reading-group" /><category term="reading-group" /><category term="ai-safety" /><category term="value-alignment" /><category term="normativity" /><summary type="html"><![CDATA[Exploring dynamic approaches to value alignment and necessary conditions for robust AI safety]]></summary></entry><entry><title type="html">Superalignment and Parallel Optimization</title><link href="https://al-folio.github.io/blog/2025/superalignment-parallel-optimization/" rel="alternate" type="text/html" title="Superalignment and Parallel Optimization" /><published>2025-03-26T00:00:00+00:00</published><updated>2025-03-26T00:00:00+00:00</updated><id>https://al-folio.github.io/blog/2025/superalignment-parallel-optimization</id><content type="html" xml:base="https://al-folio.github.io/blog/2025/superalignment-parallel-optimization/"><![CDATA[<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2503.07660">Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity</a><br />
<strong>Authors:</strong> HyunJin Kim, Xiaoyuan Yi, Jing Yao, Muhua Huang, JinYeong Bak, James Evans, Xing Xie<br />
<strong>Session Date:</strong> March 26, 2025</p>

<h2 id="abstract">Abstract</h2>

<p>The recent leap in AI capabilities, driven by big generative models, has sparked the possibility of achieving Artificial General Intelligence (AGI) and further triggered discussions on Artificial Superintelligence (ASI), a system surpassing all humans across all domains. This gives rise to the critical research question of: If we realize ASI, how do we align it with human values, ensuring it benefits rather than harms human society, a.k.a., the Superalignment problem. Despite ASI being regarded by many as solely a hypothetical concept, in this paper, we argue that superalignment is achievable and research on it should advance immediately, through simultaneous and alternating optimization of task competence and value conformity. We posit that superalignment is not merely a safeguard for ASI but also necessary for its realization. To support this position, we first provide a formal definition of superalignment rooted in the gap between capability and capacity and elaborate on our argument. Then we review existing paradigms, explore their interconnections and limitations, and illustrate a potential path to superalignment centered on two fundamental principles. We hope this work sheds light on a practical approach for developing the value-aligned next-generation AI, garnering greater benefits and reducing potential harms for humanity.</p>]]></content><author><name></name></author><category term="safety-reading-group" /><category term="reading-group" /><category term="ai-safety" /><category term="superalignment" /><category term="alignment" /><summary type="html"><![CDATA[Examining arguments for immediate superalignment research through competence and conformity optimization]]></summary></entry><entry><title type="html">Emergent Misalignment in Language Models</title><link href="https://al-folio.github.io/blog/2025/emergent-misalignment/" rel="alternate" type="text/html" title="Emergent Misalignment in Language Models" /><published>2025-03-12T00:00:00+00:00</published><updated>2025-03-12T00:00:00+00:00</updated><id>https://al-folio.github.io/blog/2025/emergent-misalignment</id><content type="html" xml:base="https://al-folio.github.io/blog/2025/emergent-misalignment/"><![CDATA[<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2310.03693">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a><br />
<strong>Authors:</strong> Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans 
<strong>Session Date:</strong> March 12, 2025</p>

<h2 id="abstract">Abstract</h2>

<p>We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It’s important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.</p>]]></content><author><name></name></author><category term="safety-reading-group" /><category term="reading-group" /><category term="ai-safety" /><category term="alignment" /><category term="finetuning" /><summary type="html"><![CDATA[Exploring how narrow finetuning can lead to broadly misaligned LLM behavior]]></summary></entry><entry><title type="html">Open Problems in Machine Unlearning for AI Safety</title><link href="https://al-folio.github.io/blog/2025/machine-unlearning-ai-safety/" rel="alternate" type="text/html" title="Open Problems in Machine Unlearning for AI Safety" /><published>2025-02-26T00:00:00+00:00</published><updated>2025-02-26T00:00:00+00:00</updated><id>https://al-folio.github.io/blog/2025/machine-unlearning-ai-safety</id><content type="html" xml:base="https://al-folio.github.io/blog/2025/machine-unlearning-ai-safety/"><![CDATA[<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2501.04952">Open Problems in Machine Unlearning for AI Safety</a><br />
<strong>Authors:</strong> Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O’Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, Yarin Gal
<strong>Session Date:</strong> February 26, 2025</p>

<h2 id="abstract">Abstract</h2>

<p>As AI systems become more capable, widely deployed, and increasingly autonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with human values is paramount. Machine unlearning – the ability to selectively forget or suppress specific types of knowledge – has shown promise for privacy and data removal tasks, which has been the primary focus of existing research. More recently, its potential application to AI safety has gained attention. In this paper, we identify key limitations that prevent unlearning from serving as a comprehensive solution for AI safety, particularly in managing dual-use knowledge in sensitive domains like cybersecurity and chemical, biological, radiological, and nuclear (CBRN) safety. In these contexts, information can be both beneficial and harmful, and models may combine seemingly harmless information for harmful purposes – unlearning this information could strongly affect beneficial uses. We provide an overview of inherent constraints and open problems, including the broader side effects of unlearning dangerous knowledge, as well as previously unexplored tensions between unlearning and existing safety mechanisms. Finally, we investigate challenges related to evaluation, robustness, and the preservation of safety features during unlearning. By mapping these limitations and open challenges, we aim to guide future research toward realistic applications of unlearning within a broader AI safety framework, acknowledging its limitations and highlighting areas where alternative approaches may be required.</p>]]></content><author><name></name></author><category term="safety-reading-group" /><category term="reading-group" /><category term="ai-safety" /><category term="machine-unlearning" /><summary type="html"><![CDATA[Our discussion of machine unlearning challenges and limitations for AI safety applications]]></summary></entry></feed>