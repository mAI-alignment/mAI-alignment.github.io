---
---

@misc{ali2025jql,
  title={Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models},
  author={Ali, Mehdi and Brack, Manuel and L{\"u}bbering, Max and Wendt, Elias and Khan, Abbas Goher and Rutmann, Richard and Jude, Alex and Kraus, Maurice and Weber, Alexander Arno and Stollenwerk, Felix and Kacz{\'e}r, David and Mai, Florian and Flek, Lucie and Sifa, Rafet and Flores-Herr, Nicolas and K{\"o}hler, Joachim and Schramowski, Patrick and Fromm, Michael and Kersting, Kristian},
  year={2025},
  eprint={2505.22232},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  abstract={High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.},
  url={https://arxiv.org/abs/2505.22232},
  bibtex_show={true},
  selected={false},
  abbr={arXiv}
}

@inproceedings{mai2025superalignment,
  title={Superalignment with Dynamic Human Values},
  author={Florian Mai and David Kacz{\'e}r and Nicholas Kluge Corr{\^e}a and Lucie Flek},
  booktitle={ICLR 2025 Workshop on Bidirectional Human-AI Alignment},
  year={2025},
  url={https://openreview.net/forum?id=WvB9hKKjSc},
  eprint={2503.13621},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  abstract={Two core challenges of alignment are 1) scalable oversight and 2) accounting for the dynamic nature of human values. While solutions like recursive reward modeling address 1), they do not simultaneously account for 2). We sketch a roadmap for a novel algorithmic framework that trains a superhuman reasoning model to decompose complex tasks into subtasks that are still amenable to human-level guidance. Our approach relies on what we call the part-to-complete generalization hypothesis, which states that the alignment of subtask solutions generalizes to the alignment of complete solutions. We advocate for the need to measure this generalization and propose ways to improve it in the future.},
  bibtex_show={true},
  selected={true},
  abbr={BiAlign}
}

@inproceedings{cornille2024learning,
  title={Learning to Plan for Language Modeling from Unlabeled Data},
  author={Nathan Cornille and Marie-Francine Moens and Florian Mai},
  booktitle={First Conference on Language Modeling},
  year={2024},
  url={https://openreview.net/forum?id=nT6fQIidrQ},
  eprint={2404.00614},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  abstract={By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. Given the textual context, this planning module learns to predict future abstract writing actions, which correspond to centroids in a clustered text embedding space. By conditioning on these actions, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.},
  bibtex_show={true},
  selected={true},
  abbr={COLM}
}

@inproceedings{mai2024improving,
  title={Improving Language Modeling by Increasing Test-time Planning Compute},
  author={Florian Mai and Nathan Cornille and Marie-Francine Moens},
  booktitle={Eighth Widening NLP Workshop (WiNLP 2024) Phase II},
  year={2024},
  url={https://openreview.net/forum?id=S3yyjW9OSY},
  abstract={Modern language models predict the next token in the sequence by considering the past text through a powerful function. However, language models have no explicit mechanism that allows them to spend computation time for planning long-distance future text, leading to a suboptimal token prediction. In this paper, we propose a planner that predicts a latent plan for many sentences into the future. By sampling multiple plans at once, we condition the language model on an accurate approximation of the distribution of text continuations, which leads to better next token prediction accuracy. In effect, this allows trading computation time for prediction accuracy.},
  bibtex_show={true},
  selected={false},
  abbr={WiNLP}
}