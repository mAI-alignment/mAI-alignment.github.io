---
---

@misc{ali2025jql,
  title={Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models},
  author={Ali, Mehdi and Brack, Manuel and L{\"u}bbering, Max and Wendt, Elias and Khan, Abbas Goher and Rutmann, Richard and Jude, Alex and Kraus, Maurice and Weber, Alexander Arno and Stollenwerk, Felix and Kacz{\'e}r, David and Mai, Florian and Flek, Lucie and Sifa, Rafet and Flores-Herr, Nicolas and K{\"o}hler, Joachim and Schramowski, Patrick and Fromm, Michael and Kersting, Kristian},
  year={2025},
  eprint={2505.22232},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  abstract={High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.},
  url={https://arxiv.org/abs/2505.22232},
  bibtex_show={true},
  selected={false},
  abbr={arXiv}
}

@misc{mai2025superalignment,
  title={Superalignment with Dynamic Human Values},
  author={Mai, Florian and Kacz{\'e}r, David and Corr{\^e}a, Nicholas Kluge and Flek, Lucie},
  year={2025},
  eprint={2503.13621},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  abstract={Two core challenges of alignment are 1) scalable oversight and 2) accounting for the dynamic nature of human values. While solutions like recursive reward modeling address 1), they do not simultaneously account for 2). We sketch a roadmap for a novel algorithmic framework that trains a superhuman reasoning model to decompose complex tasks into subtasks that are still amenable to human-level guidance. Our approach relies on what we call the part-to-complete generalization hypothesis, which states that the alignment of subtask solutions generalizes to the alignment of complete solutions. We advocate for the need to measure this generalization and propose ways to improve it in the future.},
  note={Published at the ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign)},
  url={https://arxiv.org/abs/2503.13621},
  bibtex_show={true},
  selected={true},
  abbr={BiAlign}
} 